{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXRLSAUE+yf1HK8uxPeNfw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!nvidia-smi #Check GPUs requires atleast 24GB of memory "],"metadata":{"id":"8m8dheI-ev0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get update\n","!apt-get install ninja-build #Installing Ninja (Not neccessary if you already have it installed))"],"metadata":{"id":"xmD5OdCsevra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu116 #Installing Torch to compatible version"],"metadata":{"id":"ewsziiaeevfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install click"],"metadata":{"id":"OL6-iXDlevLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scipy"],"metadata":{"id":"XMZNxjPMgNs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n","!sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n","!wget https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda-repo-ubuntu1804-11-6-local_11.6.0-510.39.01-1_amd64.deb\n","!sudo dpkg -i cuda-repo-ubuntu1804-11-6-local_11.6.0-510.39.01-1_amd64.deb\n","!sudo apt-key add /var/cuda-repo-ubuntu1804-11-6-local/7fa2af80.pub\n","!sudo apt-get update\n","!sudo apt-get -y install cuda\n","\n","#Installs CUDA"],"metadata":{"id":"14P3Vk3MgYyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get update && sudo apt-get install -y build-essential\n","#Install essential files (Run !nvidia-smi to see if GPU drivers are still installed. If not, restart instance or machine)"],"metadata":{"id":"XBDrPrUTgQOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /usr/local/cuda-11.6/lib64 | grep cusolver #Locating cusolver if exists"],"metadata":{"id":"lpmrWDoHgs9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!find /usr -iname libcudart.so* #Locating cusolver if exists"],"metadata":{"id":"OUFvoQnag4IH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#On VM, the machine couldn't locate the right path to CUDA so I had to link it manually. This step isn't required unless training.py doesn't detect CUDA to start up its plugins with\n","!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.6/lib64\n","!export CUDA_HOME=/usr/local/cuda-11.6\n","import os\n","\n","cuda_root = '/usr/local/cuda-11.6'\n","if os.path.exists(cuda_root):\n","    os.environ['CUDA_HOME'] = cuda_root\n","    os.environ['LD_LIBRARY_PATH'] = f\"{os.environ['LD_LIBRARY_PATH']}:{cuda_root}/targets/x86_64-linux/lib\"\n","    if 'LIBRARY_PATH' in os.environ:\n","        os.environ['LIBRARY_PATH'] = f\"{os.environ['LIBRARY_PATH']}:{cuda_root}/targets/x86_64-linux/lib\"\n","    else:\n","        os.environ['LIBRARY_PATH'] = f\"{cuda_root}/targets/x86_64-linux/lib\"\n","    os.environ['PATH'] = f\"{os.environ['PATH']}:{cuda_root}/bin\"\n"],"metadata":{"id":"Ryj_d4M0g94P"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvG_GXE1uvUI"},"outputs":[],"source":["#Editing grid_sample file as there is a tuple error (Try without as only the error occured at the pre-training stage)\n","content = \"\"\"\n","# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n","#\n","# NVIDIA CORPORATION and its licensors retain all intellectual property\n","# and proprietary rights in and to this software, related documentation\n","# and any modifications thereto.  Any use, reproduction, disclosure or\n","# distribution of this software and related documentation without an express\n","# license agreement from NVIDIA CORPORATION is strictly prohibited.\n","\n","#Custom replacement for `torch.nn.functional.grid_sample` that\n","#supports arbitrarily high order gradients between the input and output.\n","#Only works on 2D images and assumes\n","#`mode='bilinear'`, `padding_mode='zeros'`, `align_corners=False`.\n","\n","import torch\n","from pkg_resources import parse_version\n","\n","# pylint: disable=redefined-builtin\n","# pylint: disable=arguments-differ\n","# pylint: disable=protected-access\n","\n","#----------------------------------------------------------------------------\n","\n","enabled = False  # Enable the custom op by setting this to true.\n","_use_pytorch_1_11_api = parse_version(torch.__version__) >= parse_version('1.11.0a') # Allow prerelease builds of 1.11\n","\n","#----------------------------------------------------------------------------\n","\n","def grid_sample(input, grid):\n","    if _should_use_custom_op():\n","        return _GridSample2dForward.apply(input, grid)\n","    return torch.nn.functional.grid_sample(input=input, grid=grid, mode='bilinear', padding_mode='zeros', align_corners=False)\n","\n","#----------------------------------------------------------------------------\n","\n","def _should_use_custom_op():\n","    return enabled\n","\n","#----------------------------------------------------------------------------\n","\n","class _GridSample2dForward(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, grid):\n","        assert input.ndim == 4\n","        assert grid.ndim == 4\n","        output = torch.nn.functional.grid_sample(input=input, grid=grid, mode='bilinear', padding_mode='zeros', align_corners=False)\n","        ctx.save_for_backward(input, grid)\n","        return output\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        input, grid = ctx.saved_tensors\n","        grad_input, grad_grid = _GridSample2dBackward.apply(grad_output, input, grid)\n","        return grad_input, grad_grid\n","\n","#----------------------------------------------------------------------------\n","\n","class _GridSample2dBackward(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, grad_output, input, grid):\n","        op, _ = torch._C._jit_get_operation('aten::grid_sampler_2d_backward')\n","        if _use_pytorch_1_11_api:\n","            output_mask = (ctx.needs_input_grad[1], ctx.needs_input_grad[2])\n","            grad_input, grad_grid = op(grad_output, input, grid, 0, 0, False, output_mask)\n","        else:\n","            grad_input, grad_grid = op(grad_output, input, grid, 0, 0, False)\n","        ctx.save_for_backward(grid)\n","        return grad_input, grad_grid\n","\n","    @staticmethod\n","    def backward(ctx, grad2_grad_input, grad2_grad_grid):\n","        _ = grad2_grad_grid # unused\n","        grid, = ctx.saved_tensors\n","        grad2_grad_output = None\n","        grad2_input = None\n","        grad2_grid = None\n","\n","        if ctx.needs_input_grad[0]:\n","            grad2_grad_output = _GridSample2dForward.apply(grad2_grad_input, grid)\n","\n","        assert not ctx.needs_input_grad[2]\n","        return grad2_grad_output, grad2_input, grad2_grid\n","\n","#----------------------------------------------------------------------------\n","\"\"\"\n","\n","with open('stylegan3/torch_utils/ops/grid_sample_gradfix.py', 'w') as f:  #Replace with directory of grid_sample_gradfix.py\n","    f.write(content)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dTocfJYdI4A"},"outputs":[],"source":["#Converting Images into required format for dataset (RGB Format and 1024X1024)\n","from PIL import Image\n","import os\n","\n","def get_new_size(img, target_size):\n","    width, height = img.size\n","    aspect_ratio = width / height\n","\n","    if width > height:\n","        new_width = target_size\n","        new_height = int(target_size / aspect_ratio)\n","    else:\n","        new_width = int(target_size * aspect_ratio)\n","        new_height = target_size\n","\n","    return new_width, new_height\n","\n","def pad_image(img, target_size, padding_color):\n","    width, height = img.size\n","    result = Image.new(img.mode, (target_size, target_size), padding_color)\n","    x = (target_size - width) // 2\n","    y = (target_size - height) // 2\n","    result.paste(img, (x, y))\n","    return result\n","\n","input_directory = './your_destination_folder/resized/necklacesbatch5/home_your_destination_folder_resized_rgb_necklace' #Input folder of images\n","output_directory = 'Necklaces_Output_5' #Output folder\n","\n","if not os.path.exists(output_directory):\n","    os.makedirs(output_directory)\n","\n","padding_color = (255, 255, 255, 255)  # Change this to the desired padding color (R, G, B, A)\n","\n","for filename in os.listdir(input_directory):\n","    input_filepath = os.path.join(input_directory, filename)\n","    output_filepath = os.path.join(output_directory, os.path.splitext(filename)[0] + '.png')\n","\n","    try:\n","        with Image.open(input_filepath) as img:\n","            img = img.convert('RGB')  # Convert to PNG format\n","            new_size = get_new_size(img, 1024)\n","            img = img.resize(new_size, Image.LANCZOS)  # Resize while maintaining aspect ratio\n","            img = pad_image(img, 1024, padding_color)  # Pad the image to make it 1024x1024\n","            img.save(output_filepath, 'PNG')\n","    except Exception as e:\n","        print(f\"Error processing {filename}: {e}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsjY2PaOexrq","outputId":"df78a261-d3bd-4145-92a0-85db933df03f"},"outputs":[{"name":"stdout","output_type":"stream","text":["  0%|                                                   | 0/516 [00:00<?, ?it/s]/home/your_destination_folder/dataset_tool.py:229: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n","  img = img.resize((ww, hh), PIL.Image.LANCZOS)\n","100%|█████████████████████████████████████████| 516/516 [00:23<00:00, 22.03it/s]\n"]}],"source":["#Processing the dataset\n","\n","!python dataset_tool.py --source=./resized/necklaces --dest=data/datanecklaces\n"]},{"cell_type":"code","source":["!python train.py --outdir=outputs/necklaces-output --cfg=stylegan3-t --data=data/datanecklaces --gpus=4 --batch=8  --batch-gpu=2 --mbstd-group=2 --gamma=96 --snap=10 --mirror=1 --aug=ada --metrics=none --resume=./network-snapshot-000520.pkl\n","#Training script resuming from previous snapshot\n","#Replace no. of gpus and make sure --batch-gpu and --mbstd-group is mathematically correct with the no. of gpus and batch size.\n","#Example: Batch = 8, GPUS = 4, each gpu will take a batch (--batch-gpu) of 2.\n","\n","#--snap will determine at every 10 ticks, the network will save a snapshot of itself.\n","#replace --resume with a specific network snapshot."],"metadata":{"id":"Xf-ViAlAhSOn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cfUWjpHwiXGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"b158b966","outputId":"bfd1002d-4e7a-4960-bf46-5dd10178025d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading networks from \"./outputs/necklaces-output/00013-stylegan3-t-necklace-gpus4-batch8-gamma64/network-snapshot-000880.pkl\"...\n","Generating image for seed 0 (0/100) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n","Generating image for seed 1 (1/100) ...\n","Generating image for seed 2 (2/100) ...\n","Generating image for seed 3 (3/100) ...\n","Generating image for seed 4 (4/100) ...\n","Generating image for seed 5 (5/100) ...\n","Generating image for seed 6 (6/100) ...\n","Generating image for seed 7 (7/100) ...\n","Generating image for seed 8 (8/100) ...\n","Generating image for seed 9 (9/100) ...\n","Generating image for seed 10 (10/100) ...\n","Generating image for seed 11 (11/100) ...\n","Generating image for seed 12 (12/100) ...\n","Generating image for seed 13 (13/100) ...\n","Generating image for seed 14 (14/100) ...\n","Generating image for seed 15 (15/100) ...\n","Generating image for seed 16 (16/100) ...\n","Generating image for seed 17 (17/100) ...\n","Generating image for seed 18 (18/100) ...\n","Generating image for seed 19 (19/100) ...\n","Generating image for seed 20 (20/100) ...\n","Generating image for seed 21 (21/100) ...\n","Generating image for seed 22 (22/100) ...\n","Generating image for seed 23 (23/100) ...\n","Generating image for seed 24 (24/100) ...\n","Generating image for seed 25 (25/100) ...\n","Generating image for seed 26 (26/100) ...\n","Generating image for seed 27 (27/100) ...\n","Generating image for seed 28 (28/100) ...\n","Generating image for seed 29 (29/100) ...\n","Generating image for seed 30 (30/100) ...\n","Generating image for seed 31 (31/100) ...\n","Generating image for seed 32 (32/100) ...\n","Generating image for seed 33 (33/100) ...\n","Generating image for seed 34 (34/100) ...\n","Generating image for seed 35 (35/100) ...\n","Generating image for seed 36 (36/100) ...\n","Generating image for seed 37 (37/100) ...\n","Generating image for seed 38 (38/100) ...\n","Generating image for seed 39 (39/100) ...\n","Generating image for seed 40 (40/100) ...\n","Generating image for seed 41 (41/100) ...\n","Generating image for seed 42 (42/100) ...\n","Generating image for seed 43 (43/100) ...\n","Generating image for seed 44 (44/100) ...\n","Generating image for seed 45 (45/100) ...\n","Generating image for seed 46 (46/100) ...\n","Generating image for seed 47 (47/100) ...\n","Generating image for seed 48 (48/100) ...\n","Generating image for seed 49 (49/100) ...\n","Generating image for seed 50 (50/100) ...\n","Generating image for seed 51 (51/100) ...\n","Generating image for seed 52 (52/100) ...\n","Generating image for seed 53 (53/100) ...\n","Generating image for seed 54 (54/100) ...\n","Generating image for seed 55 (55/100) ...\n","Generating image for seed 56 (56/100) ...\n","Generating image for seed 57 (57/100) ...\n","Generating image for seed 58 (58/100) ...\n","Generating image for seed 59 (59/100) ...\n","Generating image for seed 60 (60/100) ...\n","Generating image for seed 61 (61/100) ...\n","Generating image for seed 62 (62/100) ...\n","Generating image for seed 63 (63/100) ...\n","Generating image for seed 64 (64/100) ...\n","Generating image for seed 65 (65/100) ...\n","Generating image for seed 66 (66/100) ...\n","Generating image for seed 67 (67/100) ...\n","Generating image for seed 68 (68/100) ...\n","Generating image for seed 69 (69/100) ...\n","Generating image for seed 70 (70/100) ...\n","Generating image for seed 71 (71/100) ...\n","Generating image for seed 72 (72/100) ...\n","Generating image for seed 73 (73/100) ...\n","Generating image for seed 74 (74/100) ...\n","Generating image for seed 75 (75/100) ...\n","Generating image for seed 76 (76/100) ...\n","Generating image for seed 77 (77/100) ...\n","Generating image for seed 78 (78/100) ...\n","Generating image for seed 79 (79/100) ...\n","Generating image for seed 80 (80/100) ...\n","Generating image for seed 81 (81/100) ...\n","Generating image for seed 82 (82/100) ...\n","Generating image for seed 83 (83/100) ...\n","Generating image for seed 84 (84/100) ...\n","Generating image for seed 85 (85/100) ...\n","Generating image for seed 86 (86/100) ...\n","Generating image for seed 87 (87/100) ...\n","Generating image for seed 88 (88/100) ...\n","Generating image for seed 89 (89/100) ...\n","Generating image for seed 90 (90/100) ...\n","Generating image for seed 91 (91/100) ...\n","Generating image for seed 92 (92/100) ...\n","Generating image for seed 93 (93/100) ...\n","Generating image for seed 94 (94/100) ...\n","Generating image for seed 95 (95/100) ...\n","Generating image for seed 96 (96/100) ...\n","Generating image for seed 97 (97/100) ...\n","Generating image for seed 98 (98/100) ...\n","Generating image for seed 99 (99/100) ...\n"]}],"source":["#This code allows generation of the images using the network snapshot. \n","#--trunc determines how experimental it could go from values 0-2\n","#Network should be the snapshot you want to train from.\n","\n","!python gen_images.py --outdir=out --trunc=0.7 --network=./network-snapshot-000520.pkl\n"]},{"cell_type":"code","source":["#If the gen_images.py cannot locate plugins\n","import os\n","\n","cuda_root = '/usr/local/cuda-11.6'\n","if os.path.exists(cuda_root):\n","    os.environ['CUDA_HOME'] = cuda_root\n","    os.environ['LD_LIBRARY_PATH'] = f\"{os.environ['LD_LIBRARY_PATH']}:{cuda_root}/targets/x86_64-linux/lib\"\n","    if 'LIBRARY_PATH' in os.environ:\n","        os.environ['LIBRARY_PATH'] = f\"{os.environ['LIBRARY_PATH']}:{cuda_root}/targets/x86_64-linux/lib\"\n","    else:\n","        os.environ['LIBRARY_PATH'] = f\"{cuda_root}/targets/x86_64-linux/lib\"\n","    os.environ['CPATH'] = f\"{cuda_root}/include:{os.environ.get('CPATH', '')}\"\n","    os.environ['PATH'] = f\"{os.environ['PATH']}:{cuda_root}/bin\"\n"],"metadata":{"id":"aJgrbyw3i9XG"},"execution_count":null,"outputs":[]}]}